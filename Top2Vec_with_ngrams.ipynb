{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from top2vec  import Top2Vec\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_preCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_preCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "# new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also', '(', ')', '\\'', ':', '?', ']', '[', '-', 'mr.', 'mrs.']\n",
    "lemma= WordNetLemmatizer()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].astype(str).str.lower()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x]) \n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda w: [item for item in w if len(item)>2])\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['text_tokens'].apply(lambda w: [' '.join(w) ])\n",
    "\n",
    "# do we love me or what? isn't this join thing amazing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finding bigrams and trigrams mid-covid \n",
    "\n",
    "gram_wordlist = ['worker' , 'immigration',\n",
    "                 'caregiver', 'caregivers', 'canada', 'canadian', 'temporary', 'migrant', 'foreign', 'labour', 'committee', 'family', 'immigrant', \n",
    "                 'industry', 'farm','farmer', 'skill', 'skilled', 'bill', 'resident', 'farmer', 'agriculture', 'housing',\n",
    "                   'employment', 'insurance' ,'employee' , 'harassment', 'residency' ,'residence', 'social', 'development', 'farming', 'program']\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(initialDF_preCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "bigram_scores = finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_documents(initialDF_preCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "trigram_scores = finder.score_ngrams(trigram_measures.pmi)\n",
    "bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "bigram_pmi.columns = ['bigram', 'pmi']\n",
    "bigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)\n",
    "trigram_pmi = pd.DataFrame(trigram_scores)\n",
    "trigram_pmi.columns = ['trigram', 'pmi']\n",
    "trigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)\n",
    "# Filter for bigrams with only noun-type structures\n",
    "def bigram_filter(bigram):\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in clearWords or bigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True\n",
    "# Filter for trigrams with only noun-type structures\n",
    "def trigram_filter(trigram):\n",
    "    tag = nltk.pos_tag(trigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n",
    "        return False\n",
    "    if trigram[0] in clearWords or trigram[-1] in clearWords or trigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in trigram or 't' in trigram:\n",
    "         return False\n",
    "    if 'PRON' in trigram:\n",
    "        return False\n",
    "    return True \n",
    "# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n",
    "# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n",
    "filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n",
    "                                              bigram_filter(bigram['bigram'])\\\n",
    "                                              and bigram.pmi > 5, axis = 1)][:200]\n",
    "\n",
    "filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n",
    "                                                 trigram_filter(trigram['trigram'])\\\n",
    "                                                 and trigram.pmi >1, axis = 1)][:200]\n",
    "\n",
    "\n",
    "# bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if x[0] in gram_wordlist or x[1] in gram_wordlist or x[-1] in gram_wordlist ]\n",
    "bigrams = [' '.join(x) for x in filtered_bigram.bigram.values] \n",
    "trigrams = [' '.join(x) for x in filtered_trigram.trigram.values ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Concatenate n-grams\n",
    "def replace_ngram(x): \n",
    "    #first convert to string from list, then add the bigram and trigram strings. \n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split(' ')))\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split(' ')))\n",
    "\n",
    "    print(x)\n",
    "    return x \n",
    "\n",
    "\n",
    "# Filter for only nouns adjectives and verbs\n",
    "def noun_only(x):\n",
    "    pos_comment = nltk.pos_tag(x)\n",
    "    filtered = [word[0] for word in pos_comment if word[1] in ['NN', 'NNS', 'JJ', 'RB', 'RBR', 'RBS']]\n",
    "    # to filter both noun and verbs\n",
    "    #filtered = [word[0] for word in pos_comment if word[1] in ['NN','VB', 'VBD', 'VBG','JJS','JJR', 'VBD', 'VBP', 'VBG', 'VBN', 'VBZ' 'VBN', 'VBZ']]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_preCovid['Text']= initialDF_preCovid['Text'].map(lambda x: replace_ngram(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #initialDF_midCovid['Text'] = initialDF_midCovid['Text'].apply(lambda w: [item.translate(table) for item in w])\n",
    "# initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "# initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "# initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(lambda x: [item for item in x if len(item) > 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "final_text = initialDF_preCovid.Text.map(lambda x: noun_only(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 18:37:39,713 - top2vec - INFO - Pre-processing documents for training\n",
      "2023-03-27 18:37:40,612 - top2vec - INFO - Creating joint document/word embedding\n",
      "2023-03-27 18:37:45,583 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "2023-03-27 18:38:08,098 - top2vec - INFO - Finding dense areas of documents\n",
      "2023-03-27 18:38:08,122 - top2vec - INFO - Finding topics\n"
     ]
    }
   ],
   "source": [
    "preCovid_t2v_ngrams = Top2Vec(final_text.astype(str).values, embedding_model='doc2vec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['experience', 'term', 'need', 'area', 'increase', 'day', 'well',\n",
       "        'skill', 'english', 'long', 'part', 'work', 'new', 'population',\n",
       "        'week', 'really', 'lot', 'strong', 'help', 'home', 'challenge',\n",
       "        'hard', 'atlantic_canada', 'able', 'economic', 'economy',\n",
       "        'forward', 'better', 'newcomer', 'region', 'immigration',\n",
       "        'great', 'we', 'social', 'something', 'place', 'meet', 'life',\n",
       "        'sure', 'good', 'people', 'also', 'opportunity', 'way', 'mean',\n",
       "        'future', 'important', 'employment', 'immigrant', 'care'],\n",
       "       ['deal', 'significant', 'trade_agreement', 'nafta', 'rule',\n",
       "        'trade_deal', 'cptpp', 'ndp', 'right', 'trade', 'however',\n",
       "        'already', 'tariff', 'investor', 'environmental', 'agreement',\n",
       "        'ceta', 'chapter', 'kind', 'look', 'actually', 'environment',\n",
       "        'commitment', 'clear', 'labour', 'concern', 'mean', 'export',\n",
       "        'real', 'tpp', 'party', 'canadian', 'important', 'good',\n",
       "        'sector', 'provision', 'the', 'court', 'local', 'united_state',\n",
       "        'foreign', 'get', 'today', 'first', 'world', 'market',\n",
       "        'interest', 'talk', 'liberal', 'strong'],\n",
       "       ['live', 'backlog', 'month', 'caregiver', 'in_caregiver',\n",
       "        'family', 'care', 'child', 'newcomer', 'refugee', 'centre',\n",
       "        'immigrant', 'great', 'last', 'proud', 'home', 'week',\n",
       "        'processing_time', 'filipino', 'community', 'immigration',\n",
       "        'life', 'long', 'people', 'different', 'back', 'hard', 'end',\n",
       "        'time', 'woman', 'help', 'border', 'english', 'experience',\n",
       "        'issue', 'need', 'country', 'day', 'member', 'work', 'example',\n",
       "        'citizenship', 'class', 'service', 'fact', 'well', 'citizen',\n",
       "        'application', 'lot', 'skill'],\n",
       "       ['organization', 'department', 'total', 'temporary', 'office',\n",
       "        'regard', 'much', 'average', 'mr', 'many', 'name',\n",
       "        'prime_minister', 'iii', 'broken', 'contribution', 'position',\n",
       "        'official', 'expenditure', 'individual', 'result', 'date',\n",
       "        'detail', 'amount', 'funding', 'full', 'program', 'applicant',\n",
       "        'visa', 'health', 'project', 'current', 'information',\n",
       "        'minister', 'number', 'federal', 'year', 'service', 'employee',\n",
       "        'government', 'action', 'cost', 'currently', 'first_nation',\n",
       "        'indigenous', 'processing', 'infrastructure', 'class',\n",
       "        'application', 'tax', 'money'],\n",
       "       ['caregiver', 'in_caregiver', 'backlog', 'filipino', 'report',\n",
       "        'immigration', 'live', 'processing_time', 'citizenship',\n",
       "        'immigrant', 'month', 'family', 'pipeline', 'proud', 'child',\n",
       "        'recommendation', 'newcomer', 'refugee', 'group', 'application',\n",
       "        'woman', 'atlantic_canada', 'centre', 'victim', 'last',\n",
       "        'population', 'witness', 'person', 'home', 'bill', 'committee',\n",
       "        'motion', 'amendment', 'record', 'citizen', 'bill_c', 'care',\n",
       "        'employer', 'applicant', 'action', 'skill', 'processing',\n",
       "        'people', 'requirement', 'disability', 'policy', 'previous',\n",
       "        'information', 'construction', 'employee']], dtype='<U15')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preCovid_t2v_ngrams.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 0.8892796753430459\n",
      "official 0.8783397492269205\n",
      "prime_minister 0.8666422028369025\n",
      "currently 0.856584572990688\n",
      "organization 0.8477927169998403\n",
      "last 0.8446157661274709\n",
      "contribution 0.8320069044396693\n",
      "amount 0.8140654562491578\n",
      "position 0.8134462408157496\n",
      "many 0.8040621063451541\n",
      "much 0.8026068207422012\n",
      "program 0.7994448135016263\n",
      "result 0.7968932132562376\n",
      "project 0.7929614567094898\n",
      "date 0.7924094926494736\n",
      "full 0.7893474321549279\n",
      "question 0.7873414660817412\n",
      "benefit 0.7871700374628603\n",
      "total 0.787033445037252\n",
      "iii 0.7832089417669064\n",
      "department 0.7817440356645966\n",
      "plan 0.779703790805166\n",
      "name 0.7795446805815314\n",
      "minister 0.7793770955455213\n",
      "quebec 0.7762000601414939\n",
      "expenditure 0.7737430330235139\n",
      "average 0.7714615661698017\n",
      "number 0.7704191246389597\n",
      "initiative 0.7700405691210462\n",
      "budget 0.7694348935917195\n",
      "mr 0.7685395146775065\n",
      "broken 0.7678978575862714\n",
      "security 0.7656057167228945\n",
      "individual 0.7596009915183541\n",
      "funding 0.7554252019202496\n",
      "matter 0.7536152806144073\n",
      "regard 0.7513386554370246\n",
      "reason 0.7385258050509279\n",
      "applicant 0.7331765691173564\n",
      "place 0.7288946786667548\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = preCovid_t2v_ngrams.similar_words(keywords=[\"temporary\"], keywords_neg=[], num_words=40)\n",
    "for word, score in zip(alpha, beta):\n",
    "    print(f\"{word} {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midCovid_t2v_ngrams = Top2Vec(final_text.values, embedding_model='doc2vec')\n",
    "midCovid_t2v_ngrams.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_scores, topic_nums = midCovid_t2v_ngrams.search_topics(keywords=[\"foreign_workers\"], num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c905b2281a0e18efbc7b52a3b445278246b1a358bdce3a3a9d510e8b53cc4ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
