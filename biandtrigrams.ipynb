{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_preCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_preCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also', '(', ')', ',', '\\'', ':', '.', '?', ']', '[', '-', 'mr.']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].astype(str).str.lower()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_midCovid= pd.read_csv('Hansard_mid_covid.csv', sep=',', encoding='utf-16-le')\n",
    "initialDF_midCovid.drop(['Publication', 'First Name', 'Last Name','Constituency', 'Province', 'Date', 'Time', 'Page'], axis=1)\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also', '(', ')', ',', '\\'', ':', '.', '?', ']', '[', '-', 'mr.']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].astype(str).str.lower()\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['Text'].apply(word_tokenize)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(initialDF_midCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "bigram_scores = finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_documents(initialDF_midCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "trigram_scores = finder.score_ngrams(trigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "bigram_pmi.columns = ['bigram', 'pmi']\n",
    "bigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_pmi = pd.DataFrame(trigram_scores)\n",
    "trigram_pmi.columns = ['trigram', 'pmi']\n",
    "trigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for bigrams with only noun-type structures\n",
    "def bigram_filter(bigram):\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in clearWords or bigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for trigrams with only noun-type structures\n",
    "def trigram_filter(trigram):\n",
    "    tag = nltk.pos_tag(trigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n",
    "        return False\n",
    "    if trigram[0] in clearWords or trigram[-1] in clearWords or trigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in trigram or 't' in trigram:\n",
    "         return False\n",
    "    if 'PRON' in trigram:\n",
    "        return False\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n",
    "# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n",
    "filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n",
    "                                              bigram_filter(bigram['bigram'])\\\n",
    "                                              and bigram.pmi > 7.7, axis = 1)][:200]\n",
    "\n",
    "filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n",
    "                                                 trigram_filter(trigram['trigram'])\\\n",
    "                                                 and trigram.pmi >7.7, axis = 1)][:200]\n",
    "\n",
    "\n",
    "bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n",
    "trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foreign worker program',\n",
       " 'canada emergency response',\n",
       " 'temporary foreign worker',\n",
       " 'emergency response benefit',\n",
       " 'since january 1',\n",
       " 'return tabled question',\n",
       " 'immigration refugee citizenship']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigram</th>\n",
       "      <th>pmi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(foreign, worker, program)</td>\n",
       "      <td>10.669958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(canada, emergency, response)</td>\n",
       "      <td>13.569155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(temporary, foreign, worker)</td>\n",
       "      <td>14.120216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(emergency, response, benefit)</td>\n",
       "      <td>16.239618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(since, january, 1)</td>\n",
       "      <td>18.932595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(return, tabled, question)</td>\n",
       "      <td>18.970362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(immigration, refugee, citizenship)</td>\n",
       "      <td>19.912229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               trigram        pmi\n",
       "6           (foreign, worker, program)  10.669958\n",
       "5        (canada, emergency, response)  13.569155\n",
       "4         (temporary, foreign, worker)  14.120216\n",
       "3       (emergency, response, benefit)  16.239618\n",
       "2                  (since, january, 1)  18.932595\n",
       "1           (return, tabled, question)  18.970362\n",
       "0  (immigration, refugee, citizenship)  19.912229"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_pmi.head(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Concatenate n-grams\n",
    "def replace_ngram(x):\n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split()))\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split()))\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "midcovid_ngrams = initialDF_midCovid['text_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      ['honour', 'present', 'official', 'language', ...\n",
       "1      ['man', \"'s\", 'name', 'etched', 'heart', 'name...\n",
       "2      ['would', 'take', 'opportunity', 'thank', 'pub...\n",
       "3      ['end', 'day', 'important', 'recognize', 'prog...\n",
       "4      ['pleased', 'rise', 'speak', 'bill', 'câ€‘206', ...\n",
       "                             ...                        \n",
       "236    ['pleased', 'rise', 'house', 'today', 'speak',...\n",
       "237    ['would', 'begin', 'acknowledging', 'gathered'...\n",
       "238    ['pleased', 'opportunity', 'discus', 'benefit'...\n",
       "239    ['pleasure', 'rise', 'house', 'today', 'first'...\n",
       "240    ['today', 'making', 'heartfelt', 'plea', 'hous...\n",
       "Name: text_tokens, Length: 241, dtype: object"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialDF_midCovid['text_tokens'].apply(lambda x: replace_ngram(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wednesday',\n",
       " 'federal',\n",
       " 'assistance',\n",
       " 'farmer',\n",
       " 'receiving',\n",
       " 'cover',\n",
       " 'cost',\n",
       " 'quarantine',\n",
       " 'temporary',\n",
       " 'foreign',\n",
       " 'worker',\n",
       " 'cut',\n",
       " 'half.ottawa',\n",
       " 'giving',\n",
       " 'farmer',\n",
       " 'reimbursement',\n",
       " '$',\n",
       " '1,500',\n",
       " 'apologize',\n",
       " 'forcing',\n",
       " 'cover',\n",
       " 'cost',\n",
       " 'quarantine',\n",
       " 'federal',\n",
       " 'government',\n",
       " 'cut',\n",
       " 'amount',\n",
       " '$',\n",
       " '750',\n",
       " 'farmer',\n",
       " 'still',\n",
       " 'need',\n",
       " 'bring',\n",
       " 'worker',\n",
       " 'quarantine',\n",
       " 'still',\n",
       " 'mandatory',\n",
       " 'health',\n",
       " 'measure',\n",
       " 'changed',\n",
       " 'cost',\n",
       " 'changed',\n",
       " 'either',\n",
       " 'thing',\n",
       " 'changed',\n",
       " 'ottawa',\n",
       " 'longer',\n",
       " 'part.will',\n",
       " 'minister',\n",
       " 'immigration',\n",
       " 'refugee',\n",
       " 'citizenship',\n",
       " 'immediately',\n",
       " 'reverse',\n",
       " 'cut']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initialDF_midCovid['text_tokens'][6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c905b2281a0e18efbc7b52a3b445278246b1a358bdce3a3a9d510e8b53cc4ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
