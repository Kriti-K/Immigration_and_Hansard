{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "%matplotlib inline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_preCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_preCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['the', 'said', 'like','must', 'many', 'also', 'mr.', 'madam', 'sir']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].astype(str).str.lower()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_midCovid= pd.read_csv('Hansard_mid_covid.csv', sep=',', encoding='utf-16-le')\n",
    "initialDF_midCovid.drop(['Publication', 'First Name', 'Last Name','Constituency', 'Province', 'Date', 'Time', 'Page'], axis=1)\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['the', 'said', 'like','must', 'many', 'also', 'mr.', 'madam', 'sir']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].astype(str).str.lower()\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['Text'].apply(word_tokenize)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x]) \n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda w: [item.translate(table) for item in w])\n",
    "\n",
    "# do we love me or what? isn't this join thing amazing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(initialDF_midCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "bigram_scores = finder.score_ngrams(bigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_documents(initialDF_midCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "trigram_scores = finder.score_ngrams(trigram_measures.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "bigram_pmi.columns = ['bigram', 'pmi']\n",
    "bigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_pmi = pd.DataFrame(trigram_scores)\n",
    "trigram_pmi.columns = ['trigram', 'pmi']\n",
    "trigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for bigrams with only noun-type structures\n",
    "def bigram_filter(bigram):\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in clearWords or bigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for trigrams with only noun-type structures\n",
    "def trigram_filter(trigram):\n",
    "    tag = nltk.pos_tag(trigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n",
    "        return False\n",
    "    if trigram[0] in clearWords or trigram[-1] in clearWords or trigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in trigram or 't' in trigram:\n",
    "         return False\n",
    "    if 'PRON' in trigram:\n",
    "        return False\n",
    "    return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n",
    "# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n",
    "filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n",
    "                                              bigram_filter(bigram['bigram'])\\\n",
    "                                              and bigram.pmi > 7.7, axis = 1)][:200]\n",
    "\n",
    "filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n",
    "                                                 trigram_filter(trigram['trigram'])\\\n",
    "                                                 and trigram.pmi >7.7, axis = 1)][:200]\n",
    "\n",
    "\n",
    "bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if len(x[0]) > 2 or len(x[1]) > 2]\n",
    "trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if len(x[0]) > 2 or len(x[1]) > 2 and len(x[2]) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Concatenate n-grams\n",
    "def replace_ngram(x): \n",
    "    #first convert to string from list, then add the bigram and trigram strings. \n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split(' ')))\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split(' ')))\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_midCovid.Text = initialDF_midCovid.Text.map(lambda x: replace_ngram(x))\n",
    "\n",
    "#initialDF_midCovid['Text'] = initialDF_midCovid['Text'].apply(lambda w: [item.translate(table) for item in w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].apply(word_tokenize)\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].apply(lambda x: [item for item in x if len(item) > 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only nouns adjectives and verbs\n",
    "def noun_only(x):\n",
    "    pos_comment = nltk.pos_tag(x)\n",
    "    filtered = [word[0] for word in pos_comment if word[1] in ['NN', 'NNS', 'JJ', 'VBD', 'VBP', 'VBG', 'VBN', 'VBZ']]\n",
    "    # to filter both noun and verbs\n",
    "    #filtered = [word[0] for word in pos_comment if word[1] in ['NN','VB', 'VBD', 'VBG', 'VBN', 'VBZ']]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = initialDF_midCovid.Text.map(noun_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA Model midCovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(final_text)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in final_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence = []\n",
    "for k in range(5,25):\n",
    "    print('Round: '+str(k))\n",
    "    Lda = gensim.models.ldamodel.LdaModel\n",
    "    ldamodel = Lda(doc_term_matrix, num_topics=k, id2word = dictionary, passes=20,\\\n",
    "                   iterations=100, chunksize = 1000, eval_every = None)\n",
    "    \n",
    "    cm = gensim.models.coherencemodel.CoherenceModel(model=ldamodel, texts=final_text,\\\n",
    "                                                     dictionary=dictionary, coherence='c_v')\n",
    "    coherence.append((k,cm.get_coherence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_val = [x[0] for x in coherence]\n",
    "y_val = [x[1] for x in coherence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_val,y_val)\n",
    "plt.scatter(x_val,y_val)\n",
    "plt.title('Number of Topics vs. Coherence')\n",
    "plt.xlabel('Number of Topics')\n",
    "plt.ylabel('Coherence')\n",
    "plt.xticks(x_val)\n",
    "plt.savefig('fig1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=16, id2word = dictionary, passes=35,\\\n",
    "               iterations=60,  chunksize = 1000, eval_every = None, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.show_topics(16,50, formatted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_data =  pyLDAvis.gensim.prepare(ldamodel, doc_term_matrix, dictionary, mds = 'pcoa')\n",
    "pyLDAvis.display(topic_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to extract the words for a given lambda \n",
    "\n",
    "lambd = 0.74 \n",
    "all_topics = {}\n",
    "num_terms = 14 \n",
    "\n",
    "for i in range(1,15):\n",
    "     topic = topic_data.topic_info[topic_data.topic_info.Category == 'Topic'+str(i)].copy()\n",
    "     topic['relevance'] = topic['loglift']*(1-lambd)+topic['logprob']*lambd\n",
    "     all_topics['Topic '+str(i)] = topic.sort_values(by='relevance', ascending=False).Term[:num_terms].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pd.DataFrame(all_topics).T\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c905b2281a0e18efbc7b52a3b445278246b1a358bdce3a3a9d510e8b53cc4ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
