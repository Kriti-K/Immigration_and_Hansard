{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install corextopic\n",
    "# !python -m pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from corextopic import corextopic as ct\n",
    "from corextopic import vis_topic as vt # jupyter notebooks will complain matplotlib is being loaded twice\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "import scipy.sparse as ss\n",
    "%matplotlib inline\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_preCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_preCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also', '(', ')', ',', '\\'', ':', '.', '?', ']', '[', '-', 'mr.']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].astype(str).str.lower()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x]) \n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda w: [item.translate(table) for item in w])\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "# initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x:' '.join([item for item in x if len(item)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finding bigrams and trigrams mid-covid \n",
    "\n",
    "gram_wordlist = ['worker' , 'immigration',\n",
    "                 'caregiver', 'canada', 'canadian', 'temporary', 'migrant', 'foreign', 'labour', 'committee', 'family', 'immigrant', \n",
    "                 'industry', 'farm','farmer', 'skill', 'skilled', 'bill', 'resident', 'farmer', 'agriculture', 'housing',\n",
    "                   'employment', 'insurance' ,'employee' , 'harassment', 'residency' ,'residence', 'social', 'development', 'farming', 'program']\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = nltk.collocations.BigramCollocationFinder.from_documents(initialDF_preCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "bigram_scores = finder.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = nltk.collocations.TrigramCollocationFinder.from_documents(initialDF_preCovid['text_tokens'])\n",
    "finder.apply_freq_filter(15)\n",
    "trigram_scores = finder.score_ngrams(trigram_measures.pmi)\n",
    "bigram_pmi = pd.DataFrame(bigram_scores)\n",
    "bigram_pmi.columns = ['bigram', 'pmi']\n",
    "bigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)\n",
    "trigram_pmi = pd.DataFrame(trigram_scores)\n",
    "trigram_pmi.columns = ['trigram', 'pmi']\n",
    "trigram_pmi.sort_values(by='pmi', axis = 0, ascending = True, inplace = True)\n",
    "# Filter for bigrams with only noun-type structures\n",
    "def bigram_filter(bigram):\n",
    "    tag = nltk.pos_tag(bigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['NN']:\n",
    "        return False\n",
    "    if bigram[0] in clearWords or bigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in bigram or 't' in bigram:\n",
    "        return False\n",
    "    if 'PRON' in bigram:\n",
    "        return False\n",
    "    return True\n",
    "# Filter for trigrams with only noun-type structures\n",
    "def trigram_filter(trigram):\n",
    "    tag = nltk.pos_tag(trigram)\n",
    "    if tag[0][1] not in ['JJ', 'NN'] and tag[1][1] not in ['JJ','NN']:\n",
    "        return False\n",
    "    if trigram[0] in clearWords or trigram[-1] in clearWords or trigram[1] in clearWords:\n",
    "        return False\n",
    "    if 'n' in trigram or 't' in trigram:\n",
    "         return False\n",
    "    if 'PRON' in trigram:\n",
    "        return False\n",
    "    return True \n",
    "# Can set pmi threshold to whatever makes sense - eyeball through and select threshold where n-grams stop making sense\n",
    "# choose top 500 ngrams in this case ranked by PMI that have noun like structures\n",
    "filtered_bigram = bigram_pmi[bigram_pmi.apply(lambda bigram:\\\n",
    "                                              bigram_filter(bigram['bigram'])\\\n",
    "                                              and bigram.pmi > 1, axis = 1)][:200]\n",
    "\n",
    "filtered_trigram = trigram_pmi[trigram_pmi.apply(lambda trigram: \\\n",
    "                                                 trigram_filter(trigram['trigram'])\\\n",
    "                                                 and trigram.pmi >1, axis = 1)][:200]\n",
    "\n",
    "\n",
    "bigrams = [' '.join(x) for x in filtered_bigram.bigram.values if x[0] in gram_wordlist or x[1] in gram_wordlist or x[-1] in gram_wordlist ]\n",
    "trigrams = [' '.join(x) for x in filtered_trigram.trigram.values if x[0] in gram_wordlist or x[1] in gram_wordlist or x[-1] in gram_wordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate n-grams\n",
    "def replace_ngram(x): \n",
    "    #first convert to string from list, then add the bigram and trigram strings. \n",
    "    for gram in trigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split(' ')))\n",
    "    for gram in bigrams:\n",
    "        x = x.replace(gram, '_'.join(gram.split(' ')))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for only nouns adjectives and verbs\n",
    "def noun_only(x):\n",
    "    pos_comment = nltk.pos_tag(x)\n",
    "    filtered = [word[0] for word in pos_comment if word[1] in ['NN', 'NNS', 'JJ','JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB', 'VBG', 'VBD', 'VBP']]\n",
    "    # to filter both noun and verbs\n",
    "    #filtered = [word[0] for word in pos_comment if word[1] in ['NN','VB', 'VBD', 'VBG','JJS','JJR', 'VBD', 'VBP', 'VBG', 'VBN', 'VBZ' 'VBN', 'VBZ']]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_preCovid.Text = initialDF_preCovid.Text.map(lambda x: replace_ngram(x))\n",
    "\n",
    "#initialDF_midCovid['Text'] = initialDF_midCovid['Text'].apply(lambda w: [item.translate(table) for item in w])\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].apply(lambda x: [item for item in x if len(item) > 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_text = initialDF_preCovid.Text.map(noun_only)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topic model assumes input is in the form of a doc-word matrix, where rows are documents and columns are binary counts. We'll vectorize our data, take the top 20,000 words, and convert it to a sparse matrix to save on memory usage. Note, we use binary count vectors as input to the CorEx topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['mr'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(305, 7444)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words= clearWords, max_features=8000, binary=True)\n",
    "doc_word = vectorizer.fit_transform(final_text.astype(str))\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "doc_word.shape # n_docs x m_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<305x7444 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 43022 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index (7443) out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Immigration_and_Hansard\\guidedCorEx.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m words \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(np\u001b[39m.\u001b[39masarray(vectorizer\u001b[39m.\u001b[39mget_feature_names_out(final_text)))\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m not_digit_inds \u001b[39m=\u001b[39m [ind \u001b[39mfor\u001b[39;00m ind,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word\u001b[39m.\u001b[39misdigit()]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m doc_word \u001b[39m=\u001b[39m doc_word[:,not_digit_inds]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m words    \u001b[39m=\u001b[39m [word \u001b[39mfor\u001b[39;00m ind,word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(words) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m word\u001b[39m.\u001b[39misdigit()]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m doc_word\u001b[39m.\u001b[39mshape \u001b[39m# n_docs x m_words\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\_index.py:47\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m---> 47\u001b[0m     row, col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_indices(key)\n\u001b[0;32m     49\u001b[0m     \u001b[39m# Dispatch to specialized methods.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row, INT_TYPES):\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\_index.py:168\u001b[0m, in \u001b[0;36mIndexMixin._validate_indices\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    166\u001b[0m         col \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m N\n\u001b[0;32m    167\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mslice\u001b[39m):\n\u001b[1;32m--> 168\u001b[0m     col \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_asindices(col, N)\n\u001b[0;32m    170\u001b[0m \u001b[39mreturn\u001b[39;00m row, col\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\scipy\\sparse\\_index.py:191\u001b[0m, in \u001b[0;36mIndexMixin._asindices\u001b[1;34m(self, idx, length)\u001b[0m\n\u001b[0;32m    189\u001b[0m max_indx \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmax()\n\u001b[0;32m    190\u001b[0m \u001b[39mif\u001b[39;00m max_indx \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m length:\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mindex (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m) out of range\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m max_indx)\n\u001b[0;32m    193\u001b[0m min_indx \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mmin()\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m min_indx \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: index (7443) out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names_out()))\n",
    "not_digit_inds = [ind for ind,word in enumerate(words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words    = [word for ind,word in enumerate(words) if not word.isdigit()]\n",
    "\n",
    "doc_word.shape # n_docs x m_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CorEx topic model with 50 topics\n",
    "topic_model = ct.Corex(n_hidden=21, words=words, max_iter=300, verbose=False, seed=11)\n",
    "topic_model.fit(doc_word, words=words);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: part,important,support,first,well,community,place,take,economic,state\n",
      "1: email,expectation,remain,obtain,tell,employee,stood,accept,de,profile\n",
      "2: believe,seeking,operating,outcome,delivery,duration,document,constituency,refuse,finding\n",
      "3: access,worker,riding,cultural,heritage,average,product,quarter,highest,contribution\n",
      "4: received,currently,information,service,assistance,position,system,meet,class,international\n",
      "5: challenge,mean,standard,interest,point,seek,company,drug,committee,mechanism\n",
      "6: already,isds,protect,consult,wanted,green,evening,eu,threaten,covered\n",
      "7: help,opportunity,immigrant,especially,form,newcomer,partner,focus,success,young\n",
      "8: investor,even,question,export,nafta,generation,regulation,simply,took,compensation\n",
      "9: provide,citizenship,office,meeting,provided,working,minister,accepted,common,report\n",
      "10: individual,action,heard,rather,response,minimum,still,responsible,party,recently\n",
      "11: intellectual,legal,territorial,discrimination,obligation,collective,rule,maintain,unanswered,rationale\n",
      "12: building,value,culture,existing,base,saskatchewan,transfer,deeply,production,south\n",
      "13: agreement,trade,ceta,europe,european,pacific,partnership,dispute,industry,loss\n",
      "14: union,profit,domestic,content,text,association,implement,real,refused,expense\n",
      "15: announced,grant,eligible,receive,location,loan,request,two,previous,research\n",
      "16: letter,participation,rejected,section,discussed,allocated,mark,breakdown,recipient,maintaining\n",
      "17: role,helped,demographic,nova,seem,tweet,transport,owned,legally,welcoming\n",
      "18: forward,supported,effect,always,case,president,woman,similar,entire,spirit\n",
      "19: british,columbia,pipeline,started,went,morgan,kinder,extremely,unique,gave\n",
      "20: almost,french,migrant,official,family,safe,burden,express,crossing,speaking\n"
     ]
    }
   ],
   "source": [
    "topics = topic_model.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('access', 0.1756294477457153, 1.0),\n",
       " ('worker', 0.14995232102039632, 1.0),\n",
       " ('riding', 0.1286332242542033, 1.0),\n",
       " ('cultural', 0.08850475212083216, 1.0),\n",
       " ('heritage', 0.08850475212083211, 1.0),\n",
       " ('average', 0.08465711353248635, 1.0),\n",
       " ('product', 0.0846571135324863, 1.0),\n",
       " ('quarter', 0.08042453339022203, 1.0),\n",
       " ('highest', 0.08030044291083906, 1.0),\n",
       " ('contribution', 0.07717172247810296, 1.0),\n",
       " ('join', 0.07359974446945079, 1.0),\n",
       " ('percentage', 0.07359974446945079, 1.0),\n",
       " ('political', 0.07348754065762092, 1.0),\n",
       " ('elsewhere', 0.07223358477962191, 1.0),\n",
       " ('present', 0.06591343358145012, 1.0),\n",
       " ('fiscal', 0.06541601140695334, 1.0),\n",
       " ('spent', 0.06470313526988974, 1.0),\n",
       " ('expenditure', 0.06340937669239494, 1.0),\n",
       " ('generous', 0.06340937669239494, 1.0),\n",
       " ('wage', 0.061683221230444875, 1.0),\n",
       " ('post', 0.05846747034894354, 1.0),\n",
       " ('administration', 0.057357938722852185, 1.0),\n",
       " ('length', 0.05735793872285214, 1.0),\n",
       " ('beneficial', 0.05502076033247172, 1.0),\n",
       " ('commission', 0.05502076033247172, 1.0),\n",
       " ('jurisdiction', 0.05502076033247172, 1.0),\n",
       " ('theme', 0.05502076033247172, 1.0),\n",
       " ('entity', 0.05502076033247172, 1.0),\n",
       " ('instead', 0.046992466135108664, 1.0),\n",
       " ('object', 0.04670902435959418, 1.0),\n",
       " ('delegation', 0.04670902435959418, 1.0),\n",
       " ('monthly', 0.04670902435959418, 1.0),\n",
       " ('successive', 0.04670902435959418, 1.0),\n",
       " ('stock', 0.04670902435959418, 1.0),\n",
       " ('decline', 0.04670902435959418, 1.0),\n",
       " ('facebook', 0.04670902435959418, 1.0),\n",
       " ('ready', 0.04419183450463122, 1.0),\n",
       " ('easy', 0.04254060266973806, 1.0),\n",
       " ('september', 0.041653294690816633, 1.0),\n",
       " ('negotiated', 0.04165329469081661, 1.0),\n",
       " ('agriculture', 0.040158744150429505, 1.0),\n",
       " ('obtaining', 0.03847243792818049, 1.0),\n",
       " ('modernize', 0.03847243792818049, 1.0),\n",
       " ('david', 0.03847243792818049, 1.0),\n",
       " ('transit', 0.03847243792818049, 1.0),\n",
       " ('largely', 0.03847243792818049, 1.0),\n",
       " ('occupation', 0.03847243792818049, 1.0),\n",
       " ('secret', 0.03847243792818049, 1.0),\n",
       " ('compliance', 0.03847243792818049, 1.0),\n",
       " ('park', 0.03847243792818049, 1.0),\n",
       " ('tour', 0.03847243792818049, 1.0),\n",
       " ('selling', 0.03847243792818049, 1.0),\n",
       " ('honduras', 0.03847243792818049, 1.0),\n",
       " ('carbon', 0.03847243792818049, 1.0),\n",
       " ('supporter', 0.03847243792818049, 1.0),\n",
       " ('referred', 0.03847243792818049, 1.0),\n",
       " ('fuel', 0.03847243792818049, 1.0),\n",
       " ('skilled_trades', 0.03847243792818049, 1.0),\n",
       " ('scientific', 0.03847243792818049, 1.0),\n",
       " ('occupational', 0.03847243792818049, 1.0),\n",
       " ('household', 0.03847243792818049, 1.0),\n",
       " ('labour_market_impact', 0.035453638034212316, 1.0),\n",
       " ('page', 0.03425328025910499, 1.0),\n",
       " ('scale', 0.034035427158305105, 1.0),\n",
       " ('strength', 0.034035427158305105, 1.0),\n",
       " ('portion', 0.034035427158305105, 1.0),\n",
       " ('trip', 0.034035427158305105, 1.0),\n",
       " ('flexibility', 0.03403542715830509, 1.0),\n",
       " ('indicated', 0.03403542715830509, 1.0),\n",
       " ('farm', 0.031904066803308574, 1.0),\n",
       " ('explore', 0.030309337168854908, 1.0),\n",
       " ('survey', 0.030309337168854908, 1.0),\n",
       " ('versus', 0.030309337168854908, 1.0),\n",
       " ('spoken', 0.030309337168854908, 1.0),\n",
       " ('shift', 0.030309337168854908, 1.0),\n",
       " ('village', 0.030309337168854908, 1.0),\n",
       " ('withdrew', 0.030309337168854908, 1.0),\n",
       " ('defence', 0.030309337168854908, 1.0),\n",
       " ('thereby', 0.030309337168854908, 1.0),\n",
       " ('maintenance', 0.030309337168854908, 1.0),\n",
       " ('kent', 0.030309337168854908, 1.0),\n",
       " ('relatively', 0.030309337168854908, 1.0),\n",
       " ('fresh', 0.030309337168854908, 1.0),\n",
       " ('regulated', 0.030309337168854908, 1.0),\n",
       " ('primarily', 0.030309337168854908, 1.0),\n",
       " ('ghost', 0.030309337168854908, 1.0),\n",
       " ('preserving', 0.030309337168854908, 1.0),\n",
       " ('prepare', 0.030309337168854908, 1.0),\n",
       " ('preferential', 0.030309337168854908, 1.0),\n",
       " ('impression', 0.030309337168854908, 1.0),\n",
       " ('multinational', 0.030309337168854908, 1.0),\n",
       " ('instrument', 0.030309337168854908, 1.0),\n",
       " ('salary', 0.030309337168854908, 1.0),\n",
       " ('fifth', 0.030309337168854908, 1.0),\n",
       " ('21st', 0.030309337168854908, 1.0),\n",
       " ('basement', 0.030309337168854908, 1.0),\n",
       " ('additionally', 0.030309337168854908, 1.0),\n",
       " ('aircraft', 0.030309337168854908, 1.0),\n",
       " ('adequately', 0.030309337168854908, 1.0),\n",
       " ('unemployment', 0.029319745666167886, 1.0)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topics(topic=3, n_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Anchor word not in word column labels provided to CorEx: foreign_worker_program\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: foreign_worker\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: livein_caregiver_program\n",
      "WARNING: Anchor word not in word column labels provided to CorEx: livein_caregiver\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Immigration_and_Hansard\\guidedCorEx.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m anchor_words \u001b[39m=\u001b[39m [[\u001b[39m'\u001b[39m\u001b[39mtemporary_foreign_worker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mforeign_worker_program\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mforeign_worker\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mskilled\u001b[39m\u001b[39m'\u001b[39m], [\u001b[39m'\u001b[39m\u001b[39mlivein_caregiver_program\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m'\u001b[39m\u001b[39mcaregiver\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlivein_caregiver\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m anchored_topic_modelt \u001b[39m=\u001b[39m ct\u001b[39m.\u001b[39mCorex(n_hidden\u001b[39m=\u001b[39m\u001b[39m21\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m11\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m anchored_topic_modelt\u001b[39m.\u001b[39;49mfit(doc_word, words\u001b[39m=\u001b[39;49mwords, anchors\u001b[39m=\u001b[39;49manchor_words, anchor_strength\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(anchor_words)):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/guidedCorEx.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     topic_words,_,_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39manchored_topic_modelt\u001b[39m.\u001b[39mget_topics(topic\u001b[39m=\u001b[39mn))\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\corextopic\\corextopic.py:147\u001b[0m, in \u001b[0;36mCorex.fit\u001b[1;34m(self, X, y, anchors, anchor_strength, words, docs)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, anchors\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, anchor_strength\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, words\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, docs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    144\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[39m    Fit CorEx on the data X. See fit_transform.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(X, anchors\u001b[39m=\u001b[39;49manchors, anchor_strength\u001b[39m=\u001b[39;49manchor_strength, words\u001b[39m=\u001b[39;49mwords, docs\u001b[39m=\u001b[39;49mdocs)\n\u001b[0;32m    148\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\corextopic\\corextopic.py:191\u001b[0m, in \u001b[0;36mCorex.fit_transform\u001b[1;34m(self, X, y, anchors, anchor_strength, words, docs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_theta(X, p_y_given_x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_p_y)  \u001b[39m# log p(x_i=1|y)  nv by m by k\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[39mif\u001b[39;00m nloop \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:  \u001b[39m# Structure learning step\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_alpha(X, p_y_given_x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtheta, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_p_y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtcs)\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m anchors \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m flatten(anchors):\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\corextopic\\corextopic.py:392\u001b[0m, in \u001b[0;36mCorex.calculate_alpha\u001b[1;34m(self, X, p_y_given_x, theta, log_p_y, tcs)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39m\"\"\"A rule for non-tree CorEx structure.\"\"\"\u001b[39;00m\n\u001b[0;32m    391\u001b[0m \u001b[39m# TODO: Could make it sparse also? Well, maybe not... at the beginning it's quite non-sparse\u001b[39;00m\n\u001b[1;32m--> 392\u001b[0m mis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalculate_mis(theta, log_p_y)\n\u001b[0;32m    393\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_hidden \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    394\u001b[0m     alphaopt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones((\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_visible))\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\corextopic\\corextopic.py:529\u001b[0m, in \u001b[0;36mCorex.calculate_mis\u001b[1;34m(self, theta, log_p_y)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[39m\"\"\"Return MI in nats, size n_hidden by n_variables\"\"\"\u001b[39;00m\n\u001b[0;32m    528\u001b[0m p_y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(log_p_y)\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))  \u001b[39m# size n_hidden, 1\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m mis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh_x \u001b[39m-\u001b[39m p_y \u001b[39m*\u001b[39m binary_entropy(np\u001b[39m.\u001b[39mexp(theta[\u001b[39m3\u001b[39m])\u001b[39m.\u001b[39mT) \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m p_y) \u001b[39m*\u001b[39m binary_entropy(np\u001b[39m.\u001b[39;49mexp(theta[\u001b[39m2\u001b[39;49m])\u001b[39m.\u001b[39;49mT)\n\u001b[0;32m    530\u001b[0m \u001b[39mreturn\u001b[39;00m (mis \u001b[39m-\u001b[39m \u001b[39m1.\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m2.\u001b[39m \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_samples))\u001b[39m.\u001b[39mclip(\u001b[39m0.\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\corextopic\\corextopic.py:684\u001b[0m, in \u001b[0;36mbinary_entropy\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    683\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbinary_entropy\u001b[39m(p):\n\u001b[1;32m--> 684\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mwhere(p \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m p \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mlog2(p) \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m p) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog2(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m p), \u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "anchor_words = [['temporary_foreign_worker', 'foreign_worker_program', 'foreign_worker', 'skilled'], ['livein_caregiver_program',  'caregiver', 'livein_caregiver']]\n",
    "anchored_topic_modelt = ct.Corex(n_hidden=21, seed=11)\n",
    "anchored_topic_modelt.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=4)\n",
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_,_ = zip(*anchored_topic_modelt.get_topics(topic=n))\n",
    "    print('{} '.format(n) + ','.join(topic_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worker', 0.47610059579992714, 1.0),\n",
       " ('support', 0.3420959786088661, 1.0),\n",
       " ('part', 0.33816743775790237, 1.0),\n",
       " ('important', 0.31594877919517333, 1.0),\n",
       " ('first', 0.30901666069694356, 1.0),\n",
       " ('community', 0.2990221786304724, 1.0),\n",
       " ('place', 0.28333462616836436, 1.0),\n",
       " ('canada', 0.27206822374367423, 1.0),\n",
       " ('well', 0.2688395770633176, 1.0),\n",
       " ('take', 0.2666929935123557, 1.0),\n",
       " ('economic', 0.25260773359759875, 1.0),\n",
       " ('country', 0.2480148079995145, 1.0),\n",
       " ('state', 0.2471557805380459, 1.0),\n",
       " ('opportunity', 0.24593572016228257, 1.0),\n",
       " ('life', 0.24572296154822804, 1.0),\n",
       " ('think', 0.236491930943929, 1.0),\n",
       " ('including', 0.2278585535687928, 1.0),\n",
       " ('cost', 0.22464482667507354, 1.0),\n",
       " ('issue', 0.2196815999410963, 1.0),\n",
       " ('canadian', 0.2193869981325237, 1.0),\n",
       " ('mean', 0.2193386793788577, 1.0),\n",
       " ('much', 0.21269054764041004, 1.0),\n",
       " ('want', 0.21226657261497514, 1.0),\n",
       " ('good', 0.20952545193881297, 1.0),\n",
       " ('right', 0.20594146464435484, 1.0),\n",
       " ('bill', 0.2035144938229316, 1.0),\n",
       " ('time', 0.20237147132498565, 1.0),\n",
       " ('current', 0.20141660315617557, 1.0),\n",
       " ('people', 0.2006909948327373, 1.0),\n",
       " ('house', 0.19916643906134582, 1.0),\n",
       " ('make', 0.1975858527726175, 1.0),\n",
       " ('reason', 0.1957739800465265, 1.0),\n",
       " ('provide', 0.19273179006914368, 1.0),\n",
       " ('work', 0.189757217647153, 1.0),\n",
       " ('change', 0.18919031688756602, 1.0),\n",
       " ('thing', 0.1878971130750889, 1.0),\n",
       " ('debate', 0.1851680449136955, 1.0),\n",
       " ('better', 0.18436889686422886, 1.0),\n",
       " ('term', 0.18049309784364434, 1.0),\n",
       " ('need', 0.17954280260541514, 1.0),\n",
       " ('today', 0.1782943198903524, 1.0),\n",
       " ('forward', 0.17755940085109892, 1.0),\n",
       " ('health', 0.17736484248997475, 1.0),\n",
       " ('back', 0.17685817552438324, 1.0),\n",
       " ('member', 0.17658725544398787, 1.0),\n",
       " ('able', 0.17424813007325593, 1.0),\n",
       " ('level', 0.17369273565409804, 1.0),\n",
       " ('motion', 0.17280878580014886, 1.0),\n",
       " ('united', 0.17269267828859097, 1.0),\n",
       " ('individual', 0.17226661558760833, 1.0),\n",
       " ('fact', 0.17193612704259606, 1.0),\n",
       " ('benefit', 0.17144928706796592, 1.0),\n",
       " ('world', 0.17064181579234558, 1.0),\n",
       " ('year', 0.17038394218444228, 1.0),\n",
       " ('public', 0.17026843586948573, 1.0),\n",
       " ('address', 0.16889141066512764, 1.0),\n",
       " ('however', 0.16880698141148445, 1.0),\n",
       " ('know', 0.16811988650771556, 1.0),\n",
       " ('came', 0.16697795640242, 1.0),\n",
       " ('process', 0.16652527893216887, 1.0),\n",
       " ('currently', 0.16600726658732554, 1.0),\n",
       " ('national', 0.16558735299969388, 1.0),\n",
       " ('point', 0.1638558393470553, 1.0),\n",
       " ('help', 0.16356361337457379, 1.0),\n",
       " ('come', 0.16328460391428498, 1.0),\n",
       " ('relationship', 0.15809397374102752, 1.0),\n",
       " ('significant', 0.1580849432822082, 1.0),\n",
       " ('increase', 0.15806199202717963, 1.0),\n",
       " ('study', 0.15747697364817742, 1.0),\n",
       " ('long', 0.1557966813195996, 1.0),\n",
       " ('together', 0.15476930323084234, 1.0),\n",
       " ('provincial', 0.15476930323084234, 1.0),\n",
       " ('measure', 0.15476930323084234, 1.0),\n",
       " ('specific', 0.15260996760858658, 1.0),\n",
       " ('example', 0.15136294067982217, 1.0),\n",
       " ('access', 0.1507666144616165, 1.0),\n",
       " ('order', 0.15076661446161646, 1.0),\n",
       " ('future', 0.14910540054802882, 1.0),\n",
       " ('challenge', 0.14910540054802865, 1.0),\n",
       " ('even', 0.14889877906108165, 1.0),\n",
       " ('understand', 0.14819316940998775, 1.0),\n",
       " ('effort', 0.1481931694099877, 1.0),\n",
       " ('look', 0.14728419591089972, 1.0),\n",
       " ('speak', 0.1460740183845319, 1.0),\n",
       " ('free', 0.14601301419681417, 1.0),\n",
       " ('number', 0.14585304170337696, 1.0),\n",
       " ('made', 0.14585304170337696, 1.0),\n",
       " ('small', 0.14574823360952502, 1.0),\n",
       " ('meet', 0.14521066117602952, 1.0),\n",
       " ('riding', 0.14506897510084787, 1.0),\n",
       " ('non', 0.14470483903841402, 1.0),\n",
       " ('impact', 0.14465840725089102, 1.0),\n",
       " ('increased', 0.14168934646932047, 1.0),\n",
       " ('received', 0.14168934646932047, 1.0),\n",
       " ('working', 0.14125782373549958, 1.0),\n",
       " ('government', 0.14030391983338994, 1.0),\n",
       " ('rise', 0.1397427606831291, 1.0),\n",
       " ('amount', 0.13916587934981783, 1.0),\n",
       " ('province', 0.13891870717022667, 1.0),\n",
       " ('heard', 0.13845678161359856, 1.0)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchored_topic_modelt.get_topics(topic=1, n_words=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mid covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_midCovid= pd.read_csv('Hansard_mid_covid.csv', sep=',', encoding='utf-16-le')\n",
    "initialDF_midCovid.drop(['Publication', 'First Name', 'Last Name','Constituency', 'Province', 'Date', 'Time', 'Page'], axis=1)\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].astype(str).str.lower()\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['Text'].apply(word_tokenize)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x:' '.join([item for item in x if len(item)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english', max_features=20000, binary=True)\n",
    "doc_word = vectorizer.fit_transform(initialDF_midCovid['text_tokens'].astype(str))\n",
    "doc_word = ss.csr_matrix(doc_word)\n",
    "doc_word.shape # n_docs x m_words\n",
    "\n",
    "# Get words that label the columns (needed to extract readable topics and make anchoring easier)\n",
    "words = list(np.asarray(vectorizer.get_feature_names_out()))\n",
    "not_digit_inds = [ind for ind,word in enumerate(words) if not word.isdigit()]\n",
    "doc_word = doc_word[:,not_digit_inds]\n",
    "words    = [word for ind,word in enumerate(words) if not word.isdigit()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "midcovid_unsup =  ct.Corex(n_hidden=60, words=words, max_iter=1200, verbose=False, seed=7)\n",
    "midcovid_unsup.fit(doc_word, words=words);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = midcovid_unsup.get_topics()\n",
    "for n,topic in enumerate(topics):\n",
    "    topic_words,_,_ = zip(*topic)\n",
    "    print('{}: '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midcovid_unsup.get_topics(topic=55, n_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 migrant,workers,question,season,rising,proposes,floor,flexible,reminded,middle\n"
     ]
    }
   ],
   "source": [
    "anchor_words = [['workers', 'season', 'migrant']]\n",
    "anchored_midCovidt= ct.Corex(n_hidden=45, seed=10)\n",
    "anchored_midCovidt.fit(doc_word, words=words, anchors=anchor_words, anchor_strength=3)\n",
    "for n in range(len(anchor_words)):\n",
    "    topic_words,_,_ = zip(*anchored_midCovidt.get_topics(topic=n))\n",
    "    print('{} '.format(n) + ','.join(topic_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchored_midCovidt.get_topics(topic =0, n_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print topics in text file\n"
     ]
    }
   ],
   "source": [
    "from corextopic import vis_topic as vt\n",
    "vt.vis_rep(topic_model, column_label=words, prefix='topic-model-example')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Some words never appear (or always appear)\n",
      "WARNING: Some words never appear (or always appear)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<corextopic.corextopic.Corex at 0x268cf3a5460>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_words = [['workers', 'season', 'migrant']]\n",
    "\n",
    "new_mid_covid = ct.Corex(n_hidden=75)\n",
    "new_mid_covid.fit(doc_word, words=words, anchors = anchor_words, anchor_strength=4)\n",
    "\n",
    "ncm_layer2 = ct.Corex(n_hidden = 50)\n",
    "ncm_layer2.fit(new_mid_covid.labels)    \n",
    "\n",
    "ncm_layer3 = ct.Corex(n_hidden=20)\n",
    "ncm_layer3.fit(ncm_layer2.labels)\n",
    "\n",
    "ncm_layer4 = ct.Corex(n_hidden = 4)\n",
    "ncm_layer4.fit(ncm_layer3.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight threshold is 0.210311 for graph with max of 100.000000 edges \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<networkx.classes.digraph.DiGraph at 0x268cf77ad30>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vt.vis_hierarchy([new_mid_covid, ncm_layer2, ncm_layer3, ncm_layer4],column_label=words, prefix='le' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c905b2281a0e18efbc7b52a3b445278246b1a358bdce3a3a9d510e8b53cc4ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
