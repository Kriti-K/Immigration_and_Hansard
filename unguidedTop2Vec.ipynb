{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from urllib.parse import urlencode\n",
    "import csv\n",
    "import numpy as np \n",
    "import re, itertools\n",
    "from top2vec  import Top2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_midCovid= pd.read_csv('Hansard_mid_covid.csv', sep=',', encoding='utf-16-le')\n",
    "initialDF_midCovid.drop(['Publication', 'First Name', 'Last Name','Constituency', 'Province', 'Date', 'Time', 'Page'], axis=1)\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].astype(str).str.lower()\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['Text'].apply(word_tokenize)\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "initialDF_midCovid['text_tokens'] = initialDF_midCovid['text_tokens'].apply(lambda x:' '.join([item for item in x if len(item)>3]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the text column using nltk library begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !python -m pip install top2vec[sentence_encoders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 02:53:55,102 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "c:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2023-03-02 02:53:55,500 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2023-03-02 02:54:25,806 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-02 02:54:29,404 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-02 02:54:38,740 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-02 02:54:38,794 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['worker', 'immigration', 'nafta', 'temporary', 'farmer',\n",
       "        'pandemic', 'quarantine', 'employer', 'foreign', 'labour',\n",
       "        'colleague', 'working', 'emergency', 'trafficking', 'work',\n",
       "        'farm', 'agriculture', 'term', 'another', 'issue', 'economy',\n",
       "        'wage', 'process', 'long', 'student', 'last', 'indigenous',\n",
       "        'agreement', 'part', 'government', 'economic', 'province',\n",
       "        'system', 'even', 'important', 'essential', 'security', 'broken',\n",
       "        'question', 'housing', 'quebec', 'human', 'service', 'canadian',\n",
       "        'risk', 'case', 'minister', 'plan', 'benefit', 'problem'],\n",
       "       ['immigration', 'total', 'nafta', 'indigenous', 'economy',\n",
       "        'liberal', 'risk', 'labour', 'pandemic', 'canadian', 'broken',\n",
       "        'province', 'month', 'response', 'farmer', 'quebec', 'state',\n",
       "        'last', 'agriculture', 'industry', 'speech', 'issue', 'however',\n",
       "        'world', 'supply', 'amount', 'billion', 'still', 'increase',\n",
       "        'including', 'thing', 'commitment', 'funding', 'economic',\n",
       "        'year', 'conservative', 'country', 'forward', 'long',\n",
       "        'government', 'support', 'canada', 'mean', 'family', 'everyone',\n",
       "        'minister', 'even', 'dairy', 'example', 'care']], dtype='<U12')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midCovid_unguidedModel = Top2Vec(initialDF_midCovid['text_tokens'].values, embedding_model='universal-sentence-encoder')\n",
    "midCovid_unguidedModel.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_scores = midCovid_unguidedModel.similar_words(keywords=[\"foreign\"], keywords_neg=[], num_words=40)\n",
    "for word, score in zip(words, word_scores):\n",
    "    print(f\"{word} {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['immigration', 'worker', 'nafta', 'pandemic', 'farmer', 'labour',\n",
       "        'temporary', 'quarantine', 'economy', 'colleague', 'foreign',\n",
       "        'agriculture', 'indigenous', 'farm', 'trafficking', 'issue',\n",
       "        'working', 'term', 'another', 'last', 'wage', 'province',\n",
       "        'emergency', 'long', 'broken', 'risk', 'employer', 'canadian',\n",
       "        'government', 'economic', 'work', 'student', 'quebec', 'even',\n",
       "        'agreement', 'process', 'state', 'minister', 'system', 'part',\n",
       "        'question', 'plan', 'problem', 'important', 'case', 'support',\n",
       "        'benefit', 'solution', 'crisis', 'total']], dtype='<U11')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midCovid_unguidedModel.hierarchical_topic_reduction(num_topics =1)\n",
    "midCovid_unguidedModel.topic_words_reduced"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check pre-covid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialDF_preCovid= pd.read_csv('Hansard_preCovid.csv', sep=',', encoding='utf-16-le', error_bad_lines = False) \n",
    "# # may need to change separator to include skipped lines\n",
    "# initialDF_preCovid.drop(['Publication', 'First Name', 'Last Name','Constituency', 'Province', 'Date', 'Time', 'Page'], axis=1) \n",
    "\n",
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_preCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_preCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].astype(str).str.lower()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x:' '.join([item for item in x if len(item)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 02:56:12,005 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "c:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2023-03-02 02:56:12,796 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2023-03-02 02:56:38,131 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-02 02:56:40,965 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-02 02:56:47,111 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-02 02:56:47,186 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    }
   ],
   "source": [
    "preCovid_unguidedModel = Top2Vec(initialDF_preCovid['text_tokens'].values, embedding_model='universal-sentence-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['immigration', 'immigrant', 'worker', 'nafta', 'labour',\n",
       "        'refugee', 'citizenship', 'resident', 'caregiver', 'visa',\n",
       "        'citizen', 'border', 'applicant', 'shortage', 'employment',\n",
       "        'disability', 'indigenous', 'employer', 'temporary', 'process',\n",
       "        'motion', 'parliamentary', 'initiative', 'wage', 'agreement',\n",
       "        'term', 'employee', 'skilled', 'concern', 'permanent', 'demand',\n",
       "        'foreign', 'ceta', 'significant', 'conservative', 'working',\n",
       "        'claim', 'system', 'issue', 'consultant', 'canadian', 'appeal',\n",
       "        'asylum', 'matter', 'liberal', 'backlog', 'provision', 'work',\n",
       "        'economy', 'minister'],\n",
       "       ['previous', 'shortage', 'immigration', 'debate', 'development',\n",
       "        'nafta', 'total', 'indigenous', 'backlog', 'point', 'liberal',\n",
       "        'filipino', 'economy', 'canadian', 'claim', 'province',\n",
       "        'provincial', 'hope', 'believe', 'month', 'matter', 'enough',\n",
       "        'fact', 'construction', 'global', 'broken', 'border', 'forward',\n",
       "        'year', 'labour', 'kind', 'atlantic', 'last', 'quebec',\n",
       "        'however', 'population', 'reason', 'committee', 'conservative',\n",
       "        'long', 'issue', 'heard', 'farmer', 'world', 'country', 'amount',\n",
       "        'increased', 'impact', 'industry', 'canada']], dtype='<U13')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preCovid_unguidedModel.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['immigration', 'nafta', 'immigrant', 'labour', 'shortage',\n",
       "        'refugee', 'worker', 'indigenous', 'border', 'citizenship',\n",
       "        'visa', 'applicant', 'previous', 'citizen', 'canadian',\n",
       "        'backlog', 'claim', 'temporary', 'liberal', 'agreement',\n",
       "        'matter', 'economy', 'demand', 'issue', 'conservative',\n",
       "        'initiative', 'province', 'caregiver', 'disability', 'term',\n",
       "        'development', 'resident', 'significant', 'concern', 'debate',\n",
       "        'process', 'ceta', 'construction', 'parliamentary', 'wage',\n",
       "        'employment', 'foreign', 'committee', 'long', 'motion',\n",
       "        'project', 'consultant', 'permanent', 'minister', 'system']],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preCovid_unguidedModel.hierarchical_topic_reduction(num_topics =1)\n",
    "preCovid_unguidedModel.topic_words_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'human' has not been learned by the model so it cannot be searched.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Immigration_and_Hansard\\unguidedTop2Vec.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/unguidedTop2Vec.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m alpha, beta \u001b[39m=\u001b[39m preCovid_unguidedModel\u001b[39m.\u001b[39;49msimilar_words(keywords\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m\"\u001b[39;49m], keywords_neg\u001b[39m=\u001b[39;49m[], num_words\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/unguidedTop2Vec.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m word, score \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(alpha, beta):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/unguidedTop2Vec.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\top2vec\\Top2Vec.py:2465\u001b[0m, in \u001b[0;36mTop2Vec.similar_words\u001b[1;34m(self, keywords, num_words, keywords_neg, use_index, ef)\u001b[0m\n\u001b[0;32m   2462\u001b[0m \u001b[39mif\u001b[39;00m keywords_neg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2463\u001b[0m     keywords_neg \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 2465\u001b[0m keywords, keywords_neg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_keywords(keywords, keywords_neg)\n\u001b[0;32m   2467\u001b[0m word_vecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words2word_vectors(keywords)\n\u001b[0;32m   2468\u001b[0m neg_word_vecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words2word_vectors(keywords_neg)\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\top2vec\\Top2Vec.py:1224\u001b[0m, in \u001b[0;36mTop2Vec._validate_keywords\u001b[1;34m(self, keywords, keywords_neg)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m keywords_lower \u001b[39m+\u001b[39m keywords_neg_lower:\n\u001b[0;32m   1223\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m vocab:\n\u001b[1;32m-> 1224\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has not been learned by the model so it cannot be searched.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1226\u001b[0m \u001b[39mreturn\u001b[39;00m keywords_lower, keywords_neg_lower\n",
      "\u001b[1;31mValueError\u001b[0m: 'human' has not been learned by the model so it cannot be searched."
     ]
    }
   ],
   "source": [
    "alpha, beta = preCovid_unguidedModel.similar_words(keywords=[\"human\"], keywords_neg=[], num_words=40)\n",
    "for word, score in zip(alpha, beta):\n",
    "    print(f\"{word} {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, topic_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting csv to txt file for ease of processing\n",
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_postCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_postCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "\n",
    "initialDF_postCovid['Text'] = initialDF_postCovid['Text'].astype(str).str.lower()\n",
    "initialDF_postCovid['Text'] = initialDF_postCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['Text'].apply(word_tokenize)\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['text_tokens'].apply(lambda x:' '.join([item for item in x if len(item)>3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 02:58:08,957 - top2vec - INFO - Pre-processing documents for training\n",
      "INFO:top2vec:Pre-processing documents for training\n",
      "c:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "2023-03-02 02:58:09,306 - top2vec - INFO - Downloading universal-sentence-encoder model\n",
      "INFO:top2vec:Downloading universal-sentence-encoder model\n",
      "2023-03-02 02:58:32,110 - top2vec - INFO - Creating joint document/word embedding\n",
      "INFO:top2vec:Creating joint document/word embedding\n",
      "2023-03-02 02:58:34,599 - top2vec - INFO - Creating lower dimension embedding of documents\n",
      "INFO:top2vec:Creating lower dimension embedding of documents\n",
      "2023-03-02 02:58:54,088 - top2vec - INFO - Finding dense areas of documents\n",
      "INFO:top2vec:Finding dense areas of documents\n",
      "2023-03-02 02:58:54,169 - top2vec - INFO - Finding topics\n",
      "INFO:top2vec:Finding topics\n"
     ]
    }
   ],
   "source": [
    "\n",
    "postCovid_unguidedModel = Top2Vec(initialDF_postCovid['text_tokens'].values, embedding_model='universal-sentence-encoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['in', 'xa', 'the', 'made', 'three', 'well', 'point', 'already',\n",
       "        'right', 'year', 'thing', 'question', 'help', 'still', 'we',\n",
       "        'another', 'including', 'done', 'sure', 'last', 'great', 'even',\n",
       "        'first', 'nothing', 'much', 'ircc', 'would', 'part', 'hard',\n",
       "        'bloc', 'problem', 'issue', 'world', 'mean', 'come', 'need',\n",
       "        'think', 'back', 'across', 'know', 'provide', 'going', 'take',\n",
       "        'good', 'forward', 'number', 'better', 'want', 'fact', 'example'],\n",
       "       ['immigration', 'immigrant', 'worker', 'labour', 'shortage',\n",
       "        'resident', 'quebecois', 'employer', 'temporary', 'economy',\n",
       "        'residency', 'colleague', 'foreign', 'working', 'motion', 'work',\n",
       "        'term', 'permanent', 'waiting', 'economic', 'issue', 'process',\n",
       "        'backlog', 'quebec', 'pandemic', 'farmer', 'application',\n",
       "        'election', 'agriculture', 'province', 'government', 'problem',\n",
       "        'important', 'housing', 'another', 'long', 'part', 'liberal',\n",
       "        'canadian', 'committee', 'question', 'minister', 'conservative',\n",
       "        'system', 'canada', 'solution', 'support', 'last', 'increase',\n",
       "        'permit'],\n",
       "       ['shortage', 'quebecois', 'immigration', 'point', 'backlog',\n",
       "        'economy', 'liberal', 'enough', 'labour', 'canadian', 'province',\n",
       "        'fact', 'billion', 'industry', 'pandemic', 'month', 'quebec',\n",
       "        'economic', 'long', 'last', 'issue', 'conservative', 'however',\n",
       "        'supply', 'farmer', 'inflation', 'waiting', 'immigrant', 'still',\n",
       "        'year', 'world', 'committee', 'government', 'agriculture',\n",
       "        'increase', 'minister', 'thing', 'forward', 'nothing', 'country',\n",
       "        'think', 'impact', 'including', 'problem', 'even', 'senior',\n",
       "        'canada', 'support', 'temporary', 'budget']], dtype='<U12')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "postCovid_unguidedModel.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['xa', 'point', 'in', 'the', 'made', 'well', 'three', 'question',\n",
       "        'year', 'already', 'another', 'thing', 'right', 'still', 'last',\n",
       "        'including', 'help', 'even', 'done', 'sure', 'issue', 'nothing',\n",
       "        'we', 'problem', 'part', 'first', 'great', 'bloc', 'hard',\n",
       "        'much', 'motion', 'canadian', 'think', 'number', 'provide',\n",
       "        'would', 'ircc', 'long', 'come', 'supply', 'world', 'need',\n",
       "        'application', 'solution', 'case', 'increase', 'mean', 'example',\n",
       "        'know', 'term']], dtype='<U11')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postCovid_unguidedModel.hierarchical_topic_reduction(num_topics =1)\n",
    "postCovid_unguidedModel.topic_words_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'human' has not been learned by the model so it cannot be searched.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\Immigration_and_Hansard\\unguidedTop2Vec.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/unguidedTop2Vec.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m alpha, beta \u001b[39m=\u001b[39m postCovid_unguidedModel\u001b[39m.\u001b[39;49msimilar_words(keywords\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mhuman\u001b[39;49m\u001b[39m\"\u001b[39;49m], keywords_neg\u001b[39m=\u001b[39;49m[], num_words\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/unguidedTop2Vec.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m word, score \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(alpha, beta):\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Immigration_and_Hansard/unguidedTop2Vec.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\top2vec\\Top2Vec.py:2465\u001b[0m, in \u001b[0;36mTop2Vec.similar_words\u001b[1;34m(self, keywords, num_words, keywords_neg, use_index, ef)\u001b[0m\n\u001b[0;32m   2462\u001b[0m \u001b[39mif\u001b[39;00m keywords_neg \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2463\u001b[0m     keywords_neg \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 2465\u001b[0m keywords, keywords_neg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_keywords(keywords, keywords_neg)\n\u001b[0;32m   2467\u001b[0m word_vecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words2word_vectors(keywords)\n\u001b[0;32m   2468\u001b[0m neg_word_vecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words2word_vectors(keywords_neg)\n",
      "File \u001b[1;32mc:\\Users\\Kriti Kapoor\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\top2vec\\Top2Vec.py:1224\u001b[0m, in \u001b[0;36mTop2Vec._validate_keywords\u001b[1;34m(self, keywords, keywords_neg)\u001b[0m\n\u001b[0;32m   1222\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m keywords_lower \u001b[39m+\u001b[39m keywords_neg_lower:\n\u001b[0;32m   1223\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m vocab:\n\u001b[1;32m-> 1224\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has not been learned by the model so it cannot be searched.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1226\u001b[0m \u001b[39mreturn\u001b[39;00m keywords_lower, keywords_neg_lower\n",
      "\u001b[1;31mValueError\u001b[0m: 'human' has not been learned by the model so it cannot be searched."
     ]
    }
   ],
   "source": [
    "alpha, beta = postCovid_unguidedModel.similar_words(keywords=[\"human\"], keywords_neg=[], num_words=40)\n",
    "for word, score in zip(alpha, beta):\n",
    "    print(f\"{word} {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c905b2281a0e18efbc7b52a3b445278246b1a358bdce3a3a9d510e8b53cc4ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
