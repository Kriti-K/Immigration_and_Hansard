{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from urllib.parse import urlencode\n",
    "import csv\n",
    "import numpy as np \n",
    "import re, itertools\n",
    "from top2vec  import Top2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialDF_midCovid= pd.read_csv('Hansard_mid_covid.csv', sep=',', encoding='utf-16-le')\n",
    "initialDF_midCovid.drop(['Publication', 'First Name', 'Last Name','Constituency', 'Province', 'Date', 'Time', 'Page'], axis=1)\n",
    "initialDF_midCovid['Text'] = initialDF_midCovid['Text'].str.replace(r\"^.*?Speaker,\", ' ', regex=True).astype(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the text column using nltk library begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !python -m pip install top2vec[sentence_encoders]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midCovid_unguidedModel = Top2Vec(initialDF_midCovid['Text'].values, embedding_model='universal-sentence-encoder')\n",
    "midCovid_unguidedModel.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_scores = midCovid_unguidedModel.similar_words(keywords=[\"workers\"], keywords_neg=[], num_words=40)\n",
    "for word, score in zip(words, word_scores):\n",
    "    print(f\"{word} {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_scores, topic_nums = midCovid_unguidedModel.search_topics(keywords=[\"labour\"], num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check pre-covid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialDF_preCovid= pd.read_csv('Hansard_preCovid.csv', sep=',', encoding='utf-16-le', error_bad_lines = False) \n",
    "# # may need to change separator to include skipped lines\n",
    "# initialDF_preCovid.drop(['Publication', 'First Name', 'Last Name','Constituency', 'Province', 'Date', 'Time', 'Page'], axis=1) \n",
    "\n",
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_preCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_preCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].astype(str).str.lower()\n",
    "initialDF_preCovid['Text'] = initialDF_preCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['Text'].apply(word_tokenize)\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "initialDF_preCovid['text_tokens'] = initialDF_preCovid['text_tokens'].apply(lambda x:' '.join([item for item in x if len(item)>3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preCovid_unguidedModel = Top2Vec(initialDF_preCovid['text_tokens'].values, embedding_model='universal-sentence-encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['immigration', 'immigrant', 'worker', 'nafta', 'labour',\n",
       "        'refugee', 'citizenship', 'resident', 'caregiver', 'visa',\n",
       "        'citizen', 'border', 'applicant', 'shortage', 'employment',\n",
       "        'employer', 'disability', 'indigenous', 'temporary', 'process',\n",
       "        'motion', 'parliamentary', 'wage', 'initiative', 'term',\n",
       "        'employee', 'agreement', 'concern', 'skilled', 'foreign',\n",
       "        'permanent', 'demand', 'ceta', 'significant', 'working',\n",
       "        'conservative', 'claim', 'issue', 'consultant', 'system',\n",
       "        'canadian', 'appeal', 'matter', 'provision', 'backlog',\n",
       "        'liberal', 'asylum', 'work', 'economy', 'minister'],\n",
       "       ['shortage', 'previous', 'immigration', 'debate', 'development',\n",
       "        'nafta', 'total', 'indigenous', 'backlog', 'point', 'liberal',\n",
       "        'filipino', 'economy', 'canadian', 'claim', 'province',\n",
       "        'provincial', 'hope', 'believe', 'month', 'matter', 'enough',\n",
       "        'construction', 'fact', 'broken', 'global', 'border', 'forward',\n",
       "        'year', 'kind', 'labour', 'last', 'quebec', 'atlantic',\n",
       "        'however', 'reason', 'population', 'committee', 'conservative',\n",
       "        'issue', 'long', 'heard', 'farmer', 'world', 'amount', 'country',\n",
       "        'impact', 'increased', 'industry', 'canada']], dtype='<U13')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preCovid_unguidedModel.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "immigrant 0.5515805307206525\n",
      "british 0.5200922804113817\n",
      "immigration 0.4799257633885283\n",
      "wage 0.4732187469352054\n",
      "resident 0.46973550099469835\n",
      "worker 0.4675052327334641\n",
      "american 0.4650453734989143\n",
      "international 0.46327797703190243\n",
      "national 0.46190438988333526\n",
      "employment 0.45419147782986585\n",
      "employee 0.45350021684460906\n",
      "canadian 0.4324903386632347\n",
      "expenditure 0.4315706811459571\n",
      "broken 0.4294647734017978\n",
      "government 0.41818938770041997\n",
      "conservative 0.41649454630841926\n",
      "year 0.4164675533382584\n",
      "local 0.41450124710440284\n",
      "country 0.4133913624072004\n",
      "change 0.41257050277554924\n",
      "different 0.409711326085784\n",
      "economy 0.4076529937596496\n",
      "nation 0.40698500586686226\n",
      "work 0.40370080855711643\n",
      "investment 0.4033272379705356\n",
      "citizen 0.40329748379304\n",
      "shortage 0.40192171926918646\n",
      "citizenship 0.40077371387712263\n",
      "riding 0.3995600766024652\n",
      "child 0.39836679973270783\n",
      "people 0.3981921291543734\n",
      "opportunity 0.39742322751764286\n",
      "employer 0.3972850597075456\n",
      "issue 0.395564882579488\n",
      "part 0.39101333957612155\n",
      "home 0.3886211088481176\n",
      "paid 0.38815563531820385\n",
      "concern 0.38776112528354734\n",
      "interest 0.3862422150577861\n",
      "indigenous 0.38506535954472565\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = preCovid_unguidedModel.similar_words(keywords=[\"foreign\",\"labour\"], keywords_neg=[], num_words=40)\n",
    "for word, score in zip(alpha, beta):\n",
    "    print(f\"{word} {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array(['immigration', 'immigrant', 'worker', 'nafta', 'labour', 'refugee',\n",
       "         'citizenship', 'resident', 'caregiver', 'visa', 'citizen',\n",
       "         'border', 'applicant', 'shortage', 'employment', 'employer',\n",
       "         'disability', 'indigenous', 'temporary', 'process', 'motion',\n",
       "         'parliamentary', 'wage', 'initiative', 'term', 'employee',\n",
       "         'agreement', 'concern', 'skilled', 'foreign', 'permanent',\n",
       "         'demand', 'ceta', 'significant', 'working', 'conservative',\n",
       "         'claim', 'issue', 'consultant', 'system', 'canadian', 'appeal',\n",
       "         'matter', 'provision', 'backlog', 'liberal', 'asylum', 'work',\n",
       "         'economy', 'minister'], dtype='<U13')],\n",
       " array([0.24572332]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words, topic_scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "post covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting csv to txt file for ease of processing\n",
    "newline = []\n",
    "\n",
    "textstring = ''\n",
    "with open('Hansard_postCovid.csv', 'r', encoding='utf-16-le') as inp, open('newtextfile.txt', 'w') as out:\n",
    "    for line in inp:\n",
    "      newline.append(re.findall(r'\"(.*)\"', line))\n",
    "      \n",
    "temp = pd.Series(newline)\n",
    "\n",
    "initialDF_postCovid = pd.DataFrame({'Text': temp})\n",
    "clearWords = stopwords.words('english')\n",
    "new_Stopwords = ['in', 'the', 'said', 'like','must', 'many', 'also']\n",
    "clearWords.extend(new_Stopwords)\n",
    "lemma= WordNetLemmatizer()\n",
    "\n",
    "initialDF_postCovid['Text'] = initialDF_postCovid['Text'].astype(str).str.lower()\n",
    "initialDF_postCovid['Text'] = initialDF_postCovid['Text'].str.replace(\"|\".join([r\"^.*?speaker,\", r\"^.*?chair,\" ]), ' ', regex=True)\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['Text'].apply(word_tokenize)\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['text_tokens'].apply(lambda x: [item for item in x if item not in clearWords])\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['text_tokens'].apply(lambda x: [lemma.lemmatize(item) for item in x])\n",
    "# do we love me or what? isn't this join thing amazing?\n",
    "initialDF_postCovid['text_tokens'] = initialDF_postCovid['text_tokens'].apply(lambda x:' '.join([item for item in x if len(item)>3]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postCovid_unguidedModel = Top2Vec(initialDF_postCovid['text_tokens'].values, embedding_model='universal-sentence-encoder')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "postCovid_unguidedModel.topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "permanent 0.5147940949139227\n",
      "term 0.47384104545421524\n",
      "election 0.4663639861164546\n",
      "economy 0.42325586153495187\n",
      "worker 0.4139296705635098\n",
      "opportunity 0.39194256501063085\n",
      "shortage 0.38861753023836454\n",
      "time 0.3833539235873233\n",
      "able 0.38201068492591184\n",
      "economic 0.37937424423252303\n",
      "cost 0.3713025436315296\n",
      "long 0.3698403539823836\n",
      "issue 0.35968442293537384\n",
      "immigrant 0.3574537160113743\n",
      "working 0.3484637988296785\n",
      "government 0.34830348694544594\n",
      "resident 0.34794954056206967\n",
      "riding 0.34579957302582687\n",
      "immigration 0.3452633231057951\n",
      "conservative 0.3426807328002873\n",
      "change 0.34033209593929203\n",
      "small 0.33651360210393566\n",
      "work 0.33589125663835895\n",
      "crisis 0.3348128490982276\n",
      "child 0.334699688208134\n"
     ]
    }
   ],
   "source": [
    "alpha, beta = postCovid_unguidedModel.similar_words(keywords=[\"temporary\", \"labour\"], keywords_neg=[], num_words=25)\n",
    "for word, score in zip(alpha, beta):\n",
    "    print(f\"{word} {score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c905b2281a0e18efbc7b52a3b445278246b1a358bdce3a3a9d510e8b53cc4ff4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
